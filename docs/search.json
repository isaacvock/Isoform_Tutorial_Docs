[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the EZbakR-suite isoform tutorial",
    "section": "",
    "text": "We recently put out a preprint introducing the first strategy for analyzing transcript isoform synthesis and degradation kinetics with NR-seq data from total RNA and the EZbakR-suite. To help others get this analysis strategy working in their hands, I developed this tutorial so as to show (with real data) how to go from FASTQ files to volcano plots using fastq2EZbakR to process the raw data and EZbakR to analyze the processed data. The data is hosted at this repository.\n\n\nThe dataset is a heavily downsampled version of the main NR-seq dataset presented in the preprint. It includes:\n\n3 replicates of DMSO treated, s4U fed data.\n3 replicates of SMG1 inhibitor (SMG1i) treated, s4U fed data.\n1 replicate of DMSO treated, -s4U data.\n1 replicate of SMG1i treated, -s4U data.\n\nThe fastq files included in the tutorial repo are downsampled to only include 5% of reads from a particular region of chromosome 6 (bases 16,103,264 to 57,096,698). Given how deeply the original dataset was sequenced, this leaves around 500,000 reads per fastq file. This region was chosen as it includes the SRSF3 gene, used as an example locus throughout the paper.\nSMG1 inhibition is designed to inhibit NMD. The goal of a transcript isoform-level analysis of this data is thus to identify transcripts that are stabilized upon NMD inhibition, as these are likely targets of NMD. SRSF3 is a gene that produces a well established NMD target (as well as a major isoform that is not degraded by NMD). If you do everything right, you should thus see the NMD degraded SRSF3 isoform coming up as significantly stabilized, whereas the other isoform should appear largely unaffected by SMG1i treatment.\n\n\n\nBy the end of this tutorial, you will have produced all of the output generated by fastq2EZbakR, as well as an EZbakRData object containing transcript isoform level analyses. In addition, this tutorial also showcases several other semi-orthogonal analysis strategies that can provide useful, sub-isoform level information (e.g., analyses of individual exon-exon junctions).\n\n\n\nMore details about the two main tools used in this tutorial can be found at their respective websites:\n\nfastq2EZbakR\nEZbakR"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Introduction to the EZbakR-suite isoform tutorial",
    "section": "",
    "text": "We recently put out a preprint introducing the first strategy for analyzing transcript isoform synthesis and degradation kinetics with NR-seq data from total RNA and the EZbakR-suite. To help others get this analysis strategy working in their hands, I developed this tutorial so as to show (with real data) how to go from FASTQ files to volcano plots using fastq2EZbakR to process the raw data and EZbakR to analyze the processed data. The data is hosted at this repository.\n\n\nThe dataset is a heavily downsampled version of the main NR-seq dataset presented in the preprint. It includes:\n\n3 replicates of DMSO treated, s4U fed data.\n3 replicates of SMG1 inhibitor (SMG1i) treated, s4U fed data.\n1 replicate of DMSO treated, -s4U data.\n1 replicate of SMG1i treated, -s4U data.\n\nThe fastq files included in the tutorial repo are downsampled to only include 5% of reads from a particular region of chromosome 6 (bases 16,103,264 to 57,096,698). Given how deeply the original dataset was sequenced, this leaves around 500,000 reads per fastq file. This region was chosen as it includes the SRSF3 gene, used as an example locus throughout the paper.\nSMG1 inhibition is designed to inhibit NMD. The goal of a transcript isoform-level analysis of this data is thus to identify transcripts that are stabilized upon NMD inhibition, as these are likely targets of NMD. SRSF3 is a gene that produces a well established NMD target (as well as a major isoform that is not degraded by NMD). If you do everything right, you should thus see the NMD degraded SRSF3 isoform coming up as significantly stabilized, whereas the other isoform should appear largely unaffected by SMG1i treatment.\n\n\n\nBy the end of this tutorial, you will have produced all of the output generated by fastq2EZbakR, as well as an EZbakRData object containing transcript isoform level analyses. In addition, this tutorial also showcases several other semi-orthogonal analysis strategies that can provide useful, sub-isoform level information (e.g., analyses of individual exon-exon junctions).\n\n\n\nMore details about the two main tools used in this tutorial can be found at their respective websites:\n\nfastq2EZbakR\nEZbakR"
  },
  {
    "objectID": "index.html#tutorial-setup",
    "href": "index.html#tutorial-setup",
    "title": "Introduction to the EZbakR-suite isoform tutorial",
    "section": "Tutorial setup",
    "text": "Tutorial setup\nTo get started, make git is installed on your system, clone the data repository locally, and navigate into it with:\n\ngit clone https://github.com/isaacvock/Isoform_Analysis_Tutorial.git\n\ncd Isoform_Analysis_Tutorial\n\nInside of Isoform_Analysis_Tutorial, you will find a directory called data. Unzip the GTF and FASTA files in this folder:\n\ngzip -d data/hg38_*\n\nOnce you are ready, proceed to the fastq2EZbakR portion of this tutorial!"
  },
  {
    "objectID": "EZbakR.html",
    "href": "EZbakR.html",
    "title": "EZbakR for isoform analyses",
    "section": "",
    "text": "This part of the tutorial shows you how to use EZbakR and the output of fastq2EZbakR to perform an isoform-level analysis of NR-seq data. There are steps to this analysis:\n\nCreate EZbakRData object\nEstimate TEC fraction news\nEstimate isoform fraction news\n\nLinear mixing model using TEC fraction news + isoform abundance estimates\n\nConvert fraction news to rate constants\nAverage replicate data\nCompare kinetic parameter estimates between SMG1i and DMSO samples\n\nThis tutorial will also show you how to perform and make use of alternative feature assignment strategies."
  },
  {
    "objectID": "EZbakR.html#introduction",
    "href": "EZbakR.html#introduction",
    "title": "EZbakR for isoform analyses",
    "section": "",
    "text": "This part of the tutorial shows you how to use EZbakR and the output of fastq2EZbakR to perform an isoform-level analysis of NR-seq data. There are steps to this analysis:\n\nCreate EZbakRData object\nEstimate TEC fraction news\nEstimate isoform fraction news\n\nLinear mixing model using TEC fraction news + isoform abundance estimates\n\nConvert fraction news to rate constants\nAverage replicate data\nCompare kinetic parameter estimates between SMG1i and DMSO samples\n\nThis tutorial will also show you how to perform and make use of alternative feature assignment strategies."
  },
  {
    "objectID": "EZbakR.html#isoform-level-analyses",
    "href": "EZbakR.html#isoform-level-analyses",
    "title": "EZbakR for isoform analyses",
    "section": "Isoform-level analyses",
    "text": "Isoform-level analyses\n\nQuickstart\nHere is the code that I will walk through more thoroughly in the following subsections:\n\n##### Load dependencies #####\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(EZbakR)\n\n##### Create EZbakRData object #####\n\ncB &lt;- fread(\"path/to/cB/cB.csv.gz\")\n\nmetadf &lt;- tibble(\n  sample = unique(cB$sample)\n) %&gt;%\n  dplyr::mutate(\n    tl = ifelse(grepl(\"nos4U\", sample),\n                0, 2),\n    treatment = ifelse(grepl(\"^DMSO\", sample),\n                       \"DMSO\",\n                       \"SMG1i\")\n  )\n\nezbdo &lt;- EZbakRData(cB, metadf)\n\n\n##### Estimate TEC fraction news #####\n\nezbdo &lt;- EstimateFractions(ezbdo,\n                           pold_from_nolabel = TRUE,\n                           features = c(\"XF\", \"TEC\"),\n                           filter_condition = `|`)\n\n\n##### Estimate transcript isoform fraction news #####\n\n### Load isoform quantification information\nfile_names &lt;- list.files(path = \"path/to/rsem/\",\n                         pattern = \"isoform\",\n                         full.names = TRUE)\nnames(file_names) &lt;- metadf$sample\n\n### Deconvolve isoform fraction news\n\nezbdo &lt;- ImportIsoformQuant(ezbdo,\n                            files = file_names,\n                            quant_tool = \"rsem\")\n\n\nezbdo &lt;- EstimateIsoformFractions(ezbdo)\n\n##### Estimate, average, and compare degradation rate constants #####\n\n\nezbdo &lt;- EstimateKinetics(ezbdo, features = \"transcript_id\",\n                          exactMatch = FALSE)\n\nezbdo &lt;- AverageAndRegularize(ezbdo, features = \"transcript_id\",\n                              exactMatch = FALSE)\n\nezbdo &lt;- CompareParameters(ezbdo,\n                           features = \"transcript_id\",\n                           design_factor = \"treatment\",\n                           reference = \"DMSO\",\n                           experimental = \"SMG1i\",\n                           exactMatch = FALSE)\n\n\n##### Assess results #####\n\nEZVolcanoPlot(ezbdo)\n\n\ncomparison &lt;- EZget(ezbdo,\n                    type = \"comparisons\")\n\n\ncomparison %&gt;%\n  filter(XF == \"SRSF3\")\n\n\n\nStep 1: Create EZbakRData object\nThe first step of any EZbakR analysis is to create an EZbakRData object, which consists of two components: a cB data frame and a metadf data frame. See the EZbakR docs for more details. There isn’t anything too unique in this case, except I am using some cute tricks to automatically populate the necessary metadf fields, and using data.table to load the cB due to its ultra-fast all purpose file reading function, fread():\n\n##### Load dependencies #####\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(EZbakR)\n\n##### Create EZbakRData object #####\n\ncB &lt;- fread(\"path/to/cB/cB.csv.gz\")\n\nmetadf &lt;- tibble(\n  sample = unique(cB$sample)\n) %&gt;%\n  dplyr::mutate(\n    tl = ifelse(grepl(\"nos4U\", sample),\n                0, 2),\n    treatment = ifelse(grepl(\"^DMSO\", sample),\n                       \"DMSO\",\n                       \"SMG1i\")\n  )\n\nezbdo &lt;- EZbakRData(cB, metadf)\n\nYou will have to specify the actual path to the cB file (replacing path/to/cB/ with the actual path to that directory created by fastq2EZbakR).\n\n\nStep 2: Estimate TEC fraction news\nThe first unique step of an isoform-level analysis is to estimate the fraction of reads in each transcript equivalence class (TEC) that are new. I’ll show the code first and then explain it:\n\nezbdo &lt;- EstimateFractions(ezbdo,\n                           features = c(\"XF\", \"TEC\"),\n                           filter_condition = `|`,\n                           pold_from_nolabel = TRUE)\n\nThe details are:\n\nfeatures is set to “XF” (exonic-regions of genes) and “TEC” (transcript equivalence class). The first is technically not necessary, but is included because it is very convenient to associate isoforms with their gene of origin. The second is the key feature in this case.\nfilter_condition is set to |, which means that if either the XF column or the TEC column is “__no_feature” or NA, that row will get filtered out. The default is that both have to meet this criterion (&).\npold_from_nolabel is a nice way to improve the stability of new and old read mutation rate estimates by using provided -s4U data to estimate the old read mutation rate (pold). It is not strictly necessary but can be useful when mutation rates are low or label times are short.\n\n\n\nStep 3: Estimate isoform fraction news\nNext is the real special part. EZbakR will combine information about transcript isoform abundances with the TEC fraction new estimates from last step to estimate isoform fraction news. Again, I’ll show the code (with pseudo file paths that you will have to edit) first:\n\n### Load isoform quantification information\nfile_names &lt;- list.files(path = \"path/to/rsem/\",\n                         pattern = \"isoform\",\n                         full.names = TRUE)\nnames(file_names) &lt;- metadf$sample\n\nezbdo &lt;- ImportIsoformQuant(ezbdo,\n                            files = file_names,\n                            quant_tool = \"rsem\")\n\n### Deconvolve isoform fraction news\n\nezbdo &lt;- EstimateIsoformFractions(ezbdo)\n\nThere are really two parts to this:\n\nImport isoform quantification estimates\nEstimate isoform fraction news\n\nThe first part requires you to provide a named vector of paths to all the RSEM isoform quantification files, with each file path named the metadf sample to which it comes from. I am using list.file() for this task, looking for all files in the rsem directory generated by fastq2EZbakR that have “isoform” in their name, which denotes the isoform abundance estimates from RSEM. EZbakR’s ImportIsoformQuant() function then imports this data and adds it to your EZbakRData object.\nThe second part can typically be run with default options. You may consider changing the TPM_min and count_min (1 and 10 by default) settings, which decides the TPM and expected read count cutoff for isoforms considered “expressed”. Isoforms below these cutoffs get filtered out and will not have their fraction new estimated.\n\n\nStep 4-6: Estimate, average, and compare rate constants\nFrom here on out, it’s a standard EZbakR analysis:\n\nezbdo &lt;- EstimateKinetics(ezbdo, features = \"transcript_id\",\n                          exactMatch = FALSE)\n\nezbdo &lt;- AverageAndRegularize(ezbdo, features = \"transcript_id\",\n                              exactMatch = FALSE)\n\nezbdo &lt;- CompareParameters(ezbdo,\n                           features = \"transcript_id\",\n                           design_factor = \"treatment\",\n                           reference = \"DMSO\",\n                           experimental = \"SMG1i\",\n                           exactMatch = FALSE)\n\nYou now have two different fractions tables in your EZbakRData object, so in the first step (EstimateKinetics()) you need to specify that you want the table with the feature column “transcript_id”. Setting exactMatch to FALSE prevents you from having to specify all of the features in this table (XF being the other one). I have included features = \"transcript_id\" for completeness, but it is technically overkill at this point as there is only one table of each relevant kind at each step if you have done everything as shown in this tutorial so far.\nYou can explore the output and see the stabilization of the PTC-containing SRSF3 isoform (but not the major isoform) like so:\n\nEZVolcanoPlot(ezbdo)\n\n\ncomparison &lt;- EZget(ezbdo,\n                    type = \"comparisons\")\n\n\ncomparison %&gt;%\n  filter(XF == \"SRSF3\")\n\n\n\n\nIsoform Volcano Plot"
  },
  {
    "objectID": "EZbakR.html#alternative-feature-analyses",
    "href": "EZbakR.html#alternative-feature-analyses",
    "title": "EZbakR for isoform analyses",
    "section": "Alternative feature analyses",
    "text": "Alternative feature analyses\n\nWhy?\nIsoform-level analyses are powerful strategies by which to assess the kinetics for the actual RNA species that are synthesized and degraded. That being said, for these analyses to be accurate, your annotation of expressed isoforms must be accurate. This is often difficult in practice, with troublesome, poorly annotated loci an inevitability. It can thus be nice to have ways to orthogonally validate what you are seeing by the transcript isoform level analysis.\nEnter alternative feature sets. In the fastq2EZbakR analysis, I included exon bin and exon-exon junction feature assignments, as these are both powerful options for this task. Exon-exon junction analyses can identify specific spliciing events that are correlated with a change in RNA stability, regardless of whether the full isoform splice graphs are accurate, and exon bin analyses can identify exonic regions that show strong stabilization signal. Both can be used in this case to corroborate the SRSF3 stabilization event.\n\n\nHow?\nReads will often map to several exon bins and/or exon-exon junctions. Thus, dealing with these requires some slight alterations to EstimateFractions.\nExon-bin anaysis:\n\nezbdo &lt;- EstimateFractions(ezbdo,\n                           features = c(\"XF\", \"exon_bin\"),\n                           filter_condition = `|`,\n                           split_multi_features = TRUE,\n                           multi_feature_cols = c(\"exon_bin\"),\n                           pold_from_nolabel = TRUE)\n\nOf note are the split_multi_features and multi_feature_cols options. This will copy the data for reads mapping to multiple instances of a given feature (e.g., multiple exon bins) so that fraction news are estimated for each instance of a given feature. The junction-level analysis is similar, except that there are two junction-related features:\n\nezbdo &lt;- EstimateFractions(ezbdo,\n                           features = c(\"XF\", \"junction_start\", \"junction_end\"),\n                           filter_condition = `|`,\n                           split_multi_features = TRUE,\n                           multi_feature_cols = c(\"junction_start\", \"junction_end\"),\n                           pold_from_nolabel = TRUE)\n\nThe rest of the analysis pipeline is identical, just make sure to specify the feature set to use at each step."
  },
  {
    "objectID": "fastq2EZbakR.html",
    "href": "fastq2EZbakR.html",
    "title": "fastq2EZbakR for isoform analyses",
    "section": "",
    "text": "The first step towards an isoform-level analysis of your NR-seq data is to process your data with fastq2EZbakR. Crucially, fastq2EZbakR will count mutations in your sequencing reads (necessary for all NR-seq analyses) and combine this with information as to the set of transcript isoforms in your annotation with which each read is compatible (TEC assignment; necessary for isoform-level analyses). The main output of this pipeline, which will be found in the results/cB/ after completion, will be the input for the EZbakR isoform-level analysis."
  },
  {
    "objectID": "fastq2EZbakR.html#introduction",
    "href": "fastq2EZbakR.html#introduction",
    "title": "fastq2EZbakR for isoform analyses",
    "section": "",
    "text": "The first step towards an isoform-level analysis of your NR-seq data is to process your data with fastq2EZbakR. Crucially, fastq2EZbakR will count mutations in your sequencing reads (necessary for all NR-seq analyses) and combine this with information as to the set of transcript isoforms in your annotation with which each read is compatible (TEC assignment; necessary for isoform-level analyses). The main output of this pipeline, which will be found in the results/cB/ after completion, will be the input for the EZbakR isoform-level analysis."
  },
  {
    "objectID": "fastq2EZbakR.html#running-the-pipeline",
    "href": "fastq2EZbakR.html#running-the-pipeline",
    "title": "fastq2EZbakR for isoform analyses",
    "section": "Running the pipeline",
    "text": "Running the pipeline\n\nStep 0: Setup\nfastq2EZbakR uses conda/mamba to automatically install all necessary dependencies the first time you run the pipeline. Thus, you should install one of these before running fastq2EZbakR.\nHistorically, I have suggested installing mamba over conda, as Mamba was often orders of magnitude faster than conda for fastq2EZbakR dependency installation. Recently though, conda has added support for using the same underlying solver that mamba uses (libmamba). Thus, if you already have conda installed, check out this documentation for how to configure the faster solver.\nOnce you have mamba/conda installed, create a conda environment with Snakemake and Snakedeploy installed:\n\nmamba create -c conda-forge -c bioconda --name deploy_snakemake 'snakemake&lt;8.0.0' snakedeploy\n# Or conda create ...\n\nInstalling a Snakemake version &lt; 8.0.0 is not strictly necessary for using fastq2EZbakR, but it insures compatibility with some of the HPC deployment strategies documented here.\n\n\nStep 1: Deploy\nActive your conda environment and “deploy” fastq2EZbakR like so:\n\nconda activate deploy_snakemake\nsnakedeploy deploy-workflow https://github.com/isaacvock/fastq2EZbakR.git . --branch main\n\nThis will create two new directories: config and workflow. config will include the config.yaml file that specifies what your input to the pipeline will be and sets a number of parameters that tunes how the pipeline is run. workflow contains a singular Snakefile that specifies the link at which all of the pipeline code is hosted. This is a convenient way to deploy a Snakemake pipeline, as it avoids you having to clone the entire repo and thus makes it easier to stay up-to-date with the most current pipeline version.\n\n\nStep 2: Edit the config file\nAssuming you are in the directory created when you clone the tutorial data repo as described in the intro, you should have a folder called data which shoud contain:\n\nhg38_chr6_subseq.fa: FASTA file to align reads to\nhg38_refseq_subseq.gtf: RefSeq GTF file with genomic feature coordinates, used for feature assignment of reads and transcript isoform quantification.\nfastqs: directory containing a number of sub-directories, each containing a pair of FASTQ files.\n\nIn practice, you will have to download a relevant genome FASTA file and annotation GTF file yourself. Furthermore, in the preprint we advocate for using your RNA-seq data to augment and filter a conservative reference (like RefSeq for human data; Ensembl annotations filtered for high support level isoforms would also be acceptable) with StringTie or similar tools. Our personal annotation assembly strategy (StringTie + post-hoc trimming/isoform flagging discussed in preprint and here) is implemented in a Snakemake pipeline called AnnotationCleaner.\nYou need to edit the following sections of your config file to specify input data and desired pipeline settings for isoform-level analyses:\nsamples\nPaths to FASTQ file containing directories. Each directory should contain a single FASTQ file (for single-end data) or a pair of FASTQ files (for paired-end data).\n\nsamples:\n    DMSO_8hr_1: data/fastqs/DMSO_8hr_1\n    DMSO_8hr_2: data/fastqs/DMSO_8hr_2\n    DMSO_8hr_3: data/fastqs/DMSO_8hr_3\n    DMSO_8hr_nos4U: data/fastqs/DMSO_8hr_nos4U\n    SMG1i_8hr_1: data/fastqs/SMG1i_8hr_1\n    SMG1i_8hr_2: data/fastqs/SMG1i_8hr_2\n    SMG1i_8hr_3: data/fastqs/SMG1i_8hr_3\n    SMG1i_8hr_nos4U: data/fastqs/SMG1i_8hr_nos4U\n\ncontrol_samples\nThis specifies which samples are -s4U samples and should thus be used for SNP calling. In practice, if you have lots of -s4U samples, you only need to specify 1 or 2 of them:\n\ncontrol_samples: ['SMG1i_8hr_nos4U', 'DMSO_8hr_nos4U']\n\nPE\nSpecify whether library is paired-end. Soon this will be automatically detected, but for now you need to tell fastq2EZbakR that this is a paired-end dataset\n\nPE: True\n\ngenome\nSpecify the path to the genome FASTA file. This can be relative to the directory in which you will run Snakemake (i.e., the top of the Isoform_Analysis_Tutorial direcotry), or an absolute path to the file:\n\ngenome: data/hg38_chr6_subseq.fa\n\nannotation\nSpecify the path to the genome annotation GTF file. Again, relative or absolute paths are acceptable. The example here is a relative path as in the genome example:\n\nannotation: data/hg38_refseq_chr6subseq.gtf\n\nindices\nRelative or absolute path to, and name of, directory created to contain alignment indicies. You need to use STAR (the default fastq2EZbakR aligner) for transcript-isoform level analyses and so I have chosen to name it to reflect that aligner choice:\n\nindices: data/star_chr6_subseq\n\nstrandedness\nStrandedness of your library. This data is reverse stranded, meaning that the first read in a pair represents the reverse complement of the original RNA’s sequence:\n\nstrandedness: \"reverse\"\n\nfeatures\nThis is the key part. For transcript isoform level analyses, you need to turn on assignment of reads to their transcript equivalence class (TEC; tec in config). Here, I have also turned on two other non-default feature assignment strategies: assignment of reads to exonic bins (like in DEXSeq) and exon-exon junctions. We will use these two strategies in the EZbakR portion of the tutorial as they provide complementary information to the TEC analysis:\n\nfeatures:\n  genes: True\n  exons: True\n  tec: True\n  exonic_bins: True\n  junctions: True\n  eej: False\n  eij: False\n\nflat_annotation\nScroll down a bit and you will see this parameter. Since I have specified exonic_bins to True in features, a DEXSeq flattened annotation will be created. The flat_annotation parameter allows you to specify the name and location of this annotation:\n\nflat_annotation: \"data/hg38_chr6subseq_flat.gtf\"\n\nfinal_output\nScroll down further and you will see the final_output parameter. The default will work fine here, but to get you some useful experience, I would suggest also turning on the arrow output, as this can be helpful when working with larger datasets using EZbakR. I’ll show you how in the EZbakR portion of this tutorial:\n\nfinal_output:\n  cB: True\n  cUP: False\n  arrow: True\n\nstar_sam_tags\nKeep scrolling down and you will see the star_sam_tags parameter. This specifies which additional bam file tags to include in the STAR alignment output. Since I specified the junction feature assignment (junctions: True in features), you should specify the STAR-specific junction information tags (jI and jM):\n\nstar_sam_tags: [\"NH\", \"HI\", \"AS\", \"NM\", \"MD\", \"nM\", \"jI\", \"jM\"]\n\nOther parameters\nEverything else can be left as is for this analysis, but some other parameters you may want to edit for your own analyses include:\n\nfastp_adapters and fastp_parameters: The former specifies adapter sequences for fastp to trim, and the latter specifies optional trimming parameters. I often like to do some conservative trimming, which may look like setting fastp_parameters to \"--trim_poly_x --cut_tail --cut_front --trim_tail1 3 --trim_front1 3 --trim_front2 3 --trim_tail2 3\", which hard trims the ends of all reads and also does some polyA/G and low quality base trimming.\nstar_align_params: This sets optional arguments for STAR alignment. The default config file has some standard settings that I would suggest, including lenient mismatch penalization and end-to-end alignment (rather than allowing soft-clipping). The latter is important for assignment of reads to exons, as soft-clipping can mess with this strategy (as discussed in the docs).\n\n\n\nStep 3: Run pipeline\nNavigate to the top of the Isoform_Analysis_Tutorial directory and do a dry run of the pipeline with :\n\nsnakemake -n --cores all --use-conda --rerun-triggers mtime --keep-going\n\nThe -n flag makes this a “dry run”, which means that it just checks to see if the pipeline can be run as configured, throws an informative error if it finds something wrong, and tells you all of the steps that will be run if everything looks good. You can then actually run the pipeline by removing this flag:\n\nsnakemake --cores all --use-conda --rerun-triggers mtime --keep-going\n\nWith this downsampled dataset and a 4 core laptop, the full pipeline should take about 20 minutes to run with this strategy. By default, all of the steps in the pipeline will run serially (i.e., one after another). This is often not the best way to run fastq2EZbakR, or any Snakemake pipeline. If you are running the pipeline in an HPC environment, there are several ways to get Snakemake to more effectively use all of the resources available to you, and to thus run each step of the pipeline as a separate job, with several jobs able to be queued up in parallel in many cases. See here and here for some details. If you are using Snakemake version &gt;= 8.0.0, checkout the executor plug-ins here.\n\n\nStep 4 (Optional): load tracks\nFollow the steps here to load the colored tracks created by fastq2EZbakR (found in the results/tracks directory). You can then zoom in on a particular locus, like SRSF3 to see the dramatic effect of SMG1 inhibition at particular loci\n\n\n\nSRSF3 tracks"
  },
  {
    "objectID": "fastq2EZbakR.html#next-steps",
    "href": "fastq2EZbakR.html#next-steps",
    "title": "fastq2EZbakR for isoform analyses",
    "section": "Next steps",
    "text": "Next steps\nIf everything ran smoothly, you are ready to move on to the EZbakR portion of this tutorial! The cheatsheet directory of this tutorial repo contains the important expected output, if you want to compare what you got to the “right answers”. You can also use the data in cheatsheet in case you want to skip right to the EZbakR portion without waiting for fastq2EZbakR to finish running."
  }
]